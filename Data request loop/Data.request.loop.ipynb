{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "import requests\r\n",
                "import pandas as pd\r\n",
                "from concurrent.futures import ThreadPoolExecutor, as_completed\r\n",
                "import queue\r\n",
                "import threading\r\n",
                "import re\r\n",
                "import time\r\n",
                "import os\r\n",
                "\r\n",
                "def fetch_data_with_retry(resource_id, offset, limit=1000, retries=3):\r\n",
                "    base_url = f\"https://opendata.nhsbsa.net/api/3/action/datastore_search?offset={offset}&q=%7B%22ICB_CODE%22%3A%22QMM%3A%2A%22%7D&resource_id={resource_id}&limit={limit}\"\r\n",
                "    while retries > 0:\r\n",
                "        try:\r\n",
                "            response = requests.get(base_url)\r\n",
                "            if response.status_code == 200:\r\n",
                "                data = response.json()\r\n",
                "                records = data['result']['records']\r\n",
                "                return records\r\n",
                "            elif response.status_code == 502:\r\n",
                "                print(f\"Server error 502 with offset {offset}. Retrying after 5 seconds...\")\r\n",
                "                time.sleep(5)\r\n",
                "                retries -= 1\r\n",
                "            elif response.status_code == 500:\r\n",
                "                print(f\"Server error 500 with offset {offset}.\")\r\n",
                "                return []\r\n",
                "            else:\r\n",
                "                print(f\"Error {response.status_code} with offset {offset}.\")\r\n",
                "                return []\r\n",
                "        except requests.RequestException as e:\r\n",
                "            print(f\"Request exception with offset {offset}: {e}\")\r\n",
                "            return []\r\n",
                "    print(f\"Failed to fetch data with offset {offset} after retries.\")\r\n",
                "    return []\r\n",
                "\r\n",
                "def fetch_records_for_range(resource_id, offset_queue, all_records, lock, limit, stop_event):\r\n",
                "    while not offset_queue.empty() and not stop_event.is_set():\r\n",
                "        offset = offset_queue.get()\r\n",
                "        records = fetch_data_with_retry(resource_id, offset, limit)\r\n",
                "        if not records:\r\n",
                "            stop_event.set()  # Stop signal for all workers if no records are returned\r\n",
                "            print(f\"No records found for offset {offset}. Signaling to stop further fetching.\")\r\n",
                "        else:\r\n",
                "            with lock:\r\n",
                "                all_records.extend(records)\r\n",
                "        offset_queue.task_done()\r\n",
                "\r\n",
                "def get_all_data_concurrently(resource_id, limit=1000):\r\n",
                "    all_records = []\r\n",
                "    offsets_to_fetch = queue.Queue()\r\n",
                "    stop_event = threading.Event()  # Stop signal for all workers\r\n",
                "    \r\n",
                "    # Prepopulate the offsets from 0 to 1,000,000 in steps of 1,000\r\n",
                "    for offset in range(0, 1000000, limit):\r\n",
                "        offsets_to_fetch.put(offset)\r\n",
                "    \r\n",
                "    lock = threading.Lock()\r\n",
                "\r\n",
                "    # Use ThreadPoolExecutor to fetch offsets concurrently\r\n",
                "    with ThreadPoolExecutor(max_workers=16) as executor:\r\n",
                "        futures = []\r\n",
                "        for _ in range(16):\r\n",
                "            future = executor.submit(fetch_records_for_range, resource_id, offsets_to_fetch, all_records, lock, limit, stop_event)\r\n",
                "            futures.append(future)\r\n",
                "        for future in as_completed(futures):\r\n",
                "            future.result()\r\n",
                "\r\n",
                "    return all_records\r\n",
                "\r\n",
                "def main():\r\n",
                "    # Iterate over the months from April 2019 to May 2024\r\n",
                "    start_year_month = 202205\r\n",
                "    end_year_month = 202405\r\n",
                "    \r\n",
                "    # Ensure the output directory exists\r\n",
                "    output_directory = \"Dispensing Data\"\r\n",
                "    os.makedirs(output_directory, exist_ok=True)\r\n",
                "    \r\n",
                "    current_year_month = start_year_month\r\n",
                "    while current_year_month <= end_year_month:\r\n",
                "        resource_id = f\"EPD_{current_year_month}\"\r\n",
                "        print(f\"Fetching data for {resource_id}\")\r\n",
                "        \r\n",
                "        all_data = get_all_data_concurrently(resource_id)\r\n",
                "        df = pd.DataFrame(all_data)\r\n",
                "\r\n",
                "        # Define the column order\r\n",
                "        column_order = [\r\n",
                "            'YEAR_MONTH', 'ICB_CODE', 'ICB_NAME', 'PCO_CODE', 'PCO_NAME', \r\n",
                "            'REGIONAL_OFFICE_CODE', 'REGIONAL_OFFICE_NAME', 'PRACTICE_CODE', \r\n",
                "            'PRACTICE_NAME', 'ADDRESS_1', 'ADDRESS_2', 'ADDRESS_3', 'ADDRESS_4', \r\n",
                "            'POSTCODE', 'BNF_CHAPTER_PLUS_CODE', 'BNF_CHEMICAL_SUBSTANCE', \r\n",
                "            'CHEMICAL_SUBSTANCE_BNF_DESCR', 'BNF_CODE', 'BNF_DESCRIPTION', \r\n",
                "            'QUANTITY', 'ITEMS', 'TOTAL_QUANTITY', 'ADQUSAGE', 'NIC', \r\n",
                "            'ACTUAL_COST', 'UNIDENTIFIED'\r\n",
                "        ]\r\n",
                "\r\n",
                "        # Reorder the columns in the DataFrame\r\n",
                "        df = df[column_order]\r\n",
                "\r\n",
                "        # Create the output file name based on the resource_id\r\n",
                "        yyyymm = str(current_year_month)\r\n",
                "        file_name = f\"{yyyymm} - N&WICS Prescribing Data.xlsx\"\r\n",
                "        file_path = os.path.join(output_directory, file_name)\r\n",
                "\r\n",
                "        # Save the DataFrame to an Excel file\r\n",
                "        df.to_excel(file_path, index=False)\r\n",
                "        print(f\"Data has been fetched and saved to {file_path}\")\r\n",
                "\r\n",
                "        # Increment the year_month\r\n",
                "        year = current_year_month // 100\r\n",
                "        month = current_year_month % 100\r\n",
                "        if month == 12:\r\n",
                "            year += 1\r\n",
                "            month = 1\r\n",
                "        else:\r\n",
                "            month += 1\r\n",
                "        current_year_month = year * 100 + month\r\n",
                "\r\n",
                "if __name__ == \"__main__\":\r\n",
                "    main()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6ca0f43c-0898-4c67-9ab0-6f7d0be6717b",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Fetching data for EPD_202205\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 13000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 12000\nFinished processing offset 5000\nFinished processing offset 11000\nFinished processing offset 0\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 10000\nFinished processing offset 6000\nFinished processing offset 4000\nFinished processing offset 3000\nFinished processing offset 9000\nFinished processing offset 1000",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\nFinished processing offset 14000\nFinished processing offset 7000\nFinished processing offset 2000\nFinished processing offset 15000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 8000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 19000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 18000\nFinished processing offset 16000\nFinished processing offset 21000\nFinished processing offset 25000\nFinished processing offset 17000",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\nFinished processing offset 29000\nFinished processing offset 27000\nFinished processing offset 31000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 20000\nFinished processing offset 23000\nFinished processing offset 26000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 24000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Finished processing offset 32000\nFinished processing offset 28000\nFinished processing offset 22000\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": null
        }
    ]
}