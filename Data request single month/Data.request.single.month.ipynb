{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "6ca0f43c-0898-4c67-9ab0-6f7d0be6717b",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fetching data for EPD_202205\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 13000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 12000\n",
                        "Finished processing offset 5000\n",
                        "Finished processing offset 11000\n",
                        "Finished processing offset 0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 10000\n",
                        "Finished processing offset 6000\n",
                        "Finished processing offset 4000\n",
                        "Finished processing offset 3000\n",
                        "Finished processing offset 9000\n",
                        "Finished processing offset 1000"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Finished processing offset 14000\n",
                        "Finished processing offset 7000\n",
                        "Finished processing offset 2000\n",
                        "Finished processing offset 15000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 8000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 19000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 18000\n",
                        "Finished processing offset 16000\n",
                        "Finished processing offset 21000\n",
                        "Finished processing offset 25000\n",
                        "Finished processing offset 17000"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Finished processing offset 29000\n",
                        "Finished processing offset 27000\n",
                        "Finished processing offset 31000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 20000\n",
                        "Finished processing offset 23000\n",
                        "Finished processing offset 26000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 24000\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finished processing offset 32000\n",
                        "Finished processing offset 28000\n",
                        "Finished processing offset 22000\n"
                    ]
                }
            ],
            "source": [
                "import requests\n",
                "import pandas as pd\n",
                "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                "import queue\n",
                "import threading\n",
                "import re\n",
                "import time\n",
                "import os\n",
                "\n",
                "def fetch_data_with_retry(resource_id, offset, limit=1000, retries=3):\n",
                "    base_url = f\"https://opendata.nhsbsa.net/api/3/action/datastore_search?offset={offset}&q=%7B%22ICB_CODE%22%3A%22QMM%3A%2A%22%7D&resource_id={resource_id}&limit={limit}\"\n",
                "    while retries > 0:\n",
                "        try:\n",
                "            response = requests.get(base_url)\n",
                "            if response.status_code == 200:\n",
                "                data = response.json()\n",
                "                records = data['result']['records']\n",
                "                return records\n",
                "            elif response.status_code == 502:\n",
                "                print(f\"Server error 502 with offset {offset}. Retrying after 5 seconds...\")\n",
                "                time.sleep(5)\n",
                "                retries -= 1\n",
                "            elif response.status_code == 500:\n",
                "                print(f\"Server error 500 with offset {offset}.\")\n",
                "                return []\n",
                "            else:\n",
                "                print(f\"Error {response.status_code} with offset {offset}.\")\n",
                "                return []\n",
                "        except requests.RequestException as e:\n",
                "            print(f\"Request exception with offset {offset}: {e}\")\n",
                "            return []\n",
                "    print(f\"Failed to fetch data with offset {offset} after retries.\")\n",
                "    return []\n",
                "\n",
                "def fetch_records_for_range(resource_id, offset_queue, all_records, lock, limit, stop_event):\n",
                "    while not offset_queue.empty() and not stop_event.is_set():\n",
                "        offset = offset_queue.get()\n",
                "        records = fetch_data_with_retry(resource_id, offset, limit)\n",
                "        if not records:\n",
                "            stop_event.set()  # Stop signal for all workers if no records are returned\n",
                "            print(f\"No records found for offset {offset}. Signaling to stop further fetching.\")\n",
                "        else:\n",
                "            with lock:\n",
                "                all_records.extend(records)\n",
                "        offset_queue.task_done()\n",
                "\n",
                "def get_all_data_concurrently(resource_id, limit=1000):\n",
                "    all_records = []\n",
                "    offsets_to_fetch = queue.Queue()\n",
                "    stop_event = threading.Event()  # Stop signal for all workers\n",
                "    \n",
                "    # Prepopulate the offsets from 0 to 1,000,000 in steps of 1,000\n",
                "    for offset in range(0, 1000000, limit):\n",
                "        offsets_to_fetch.put(offset)\n",
                "    \n",
                "    lock = threading.Lock()\n",
                "\n",
                "    # Use ThreadPoolExecutor to fetch offsets concurrently\n",
                "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
                "        futures = []\n",
                "        for _ in range(16):\n",
                "            future = executor.submit(fetch_records_for_range, resource_id, offsets_to_fetch, all_records, lock, limit, stop_event)\n",
                "            futures.append(future)\n",
                "        for future in as_completed(futures):\n",
                "            future.result()\n",
                "\n",
                "    return all_records\n",
                "\n",
                "def main():\n",
                "    # Set YYYYMM data set to pull\n",
                "    year_month = 202405\n",
                "    \n",
                "    # Ensure the output directory exists\n",
                "    output_directory = \"Dispensing Data\"\n",
                "    os.makedirs(output_directory, exist_ok=True)\n",
                "    \n",
                "    resource_id = f\"EPD_{year_month}\"\n",
                "    print(f\"Fetching data for {resource_id}\")\n",
                "    \n",
                "    all_data = get_all_data_concurrently(resource_id)\n",
                "    df = pd.DataFrame(all_data)\n",
                "\n",
                "    # Define the column order\n",
                "    column_order = [\n",
                "        'YEAR_MONTH', 'ICB_CODE', 'ICB_NAME', 'PCO_CODE', 'PCO_NAME', \n",
                "        'REGIONAL_OFFICE_CODE', 'REGIONAL_OFFICE_NAME', 'PRACTICE_CODE', \n",
                "        'PRACTICE_NAME', 'ADDRESS_1', 'ADDRESS_2', 'ADDRESS_3', 'ADDRESS_4', \n",
                "        'POSTCODE', 'BNF_CHAPTER_PLUS_CODE', 'BNF_CHEMICAL_SUBSTANCE', \n",
                "        'CHEMICAL_SUBSTANCE_BNF_DESCR', 'BNF_CODE', 'BNF_DESCRIPTION', \n",
                "        'QUANTITY', 'ITEMS', 'TOTAL_QUANTITY', 'ADQUSAGE', 'NIC', \n",
                "        'ACTUAL_COST', 'UNIDENTIFIED'\n",
                "    ]\n",
                "\n",
                "    # Reorder the columns in the DataFrame\n",
                "    df = df[column_order]\n",
                "\n",
                "    # Create the output file name based on the resource_id\n",
                "    yyyymm = str(year_month)\n",
                "    file_name = f\"{yyyymm} - N&WICS Prescribing Data.xlsx\"\n",
                "    file_path = os.path.join(output_directory, file_name)\n",
                "\n",
                "    # Save the DataFrame to an Excel file\n",
                "    df.to_excel(file_path, index=False)\n",
                "    print(f\"Data has been fetched and saved to {file_path}\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
